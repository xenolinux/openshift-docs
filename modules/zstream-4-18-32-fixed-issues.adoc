// Module included in the following assemblies:
//
// * release_notes/ocp-4-18-release-notes.adoc

:_mod-docs-content-type: REFERENCE
[id="zstream-4-18-32-fixed-issues_{context}"]
= Fixed issues

[role="_abstract"]
You can ensure your environment remains stable by reviewing these fixed issues to verify resolved bugs.

The following issues are fixed for this release:

* Before this update, when service endpoints were deleted and recreated in {product-title} clusters using OVN-Kubernetes networking and the service port differed from the endpoint port, stale User Datagram Protocol (UDP) connection tracking (conntrack) entries could remain on compute nodes. This occurred because the `conntrack` cleanup logic incorrectly used the endpoint port, which is the target port on the pod, instead of the externally-facing service port that clients connect to when attempting to delete stale entries. With this release, the cleanup process uses the service port when deleting or updating service endpoints. This change ensures that stale `conntrack` entries are correctly matched and removed. Network connectivity now remains reliable during service endpoint lifecycle changes. (link:https://issues.redhat.com/browse/OCPBUGS-70346[OCPBUGS-70346])

* Before this update, a runtime error prevented an egress IP address from being created in an {product-title} cluster on {azure-first}. With this release, a code fix resolves the runtime error. As a result, an egress IP address can be created in an {product-title}t cluster on {azure-short}. (link:https://issues.redhat.com/browse/OCPBUGS-73750[OCPBUGS-73750])

* Before this update, `iptables-alerter` pods experienced high CPU usage in some clusters due to an issue in the 4.18.20 upgrade. As a consequence, high CPU usage impacted `iptables-alerter` pods, causing performance degradation. With this release, iptables-alerter CPU usage has been reduced by optimizing code in version 4.18.21. As a result, high CPU usage for `iptables-alerter` pods has been resolved, improving cluster performance. (link:https://issues.redhat.com/browse/OCPBUGS-73767[OCPBUGS-73767])

* Before this update, {aws-first} APIs returned inconsistent results regarding the existence of a `Machine`. The safeguards designed to handle this inconsistency checked the stored instance ID in the incorrect location. As a consequence, during {aws-short} API instability, virtual machines (VMs) leaked and attempted to join the cluster indefinitely. With this release, the system uses the correct provider ID for consistency checks. If an instance does not appear within 20 seconds, the machine status changes to `Failed` to prevent instance leaks. (link:https://issues.redhat.com/browse/OCPBUGS-73789[OCPBUGS-73789])

* Before this update, if etcd shared a volume with OLM, disk contention might occur. With this release, the default catalog polling interval is increased from ten minutes to four hours. As a result, some instances of catalog pod startup probe failures are fixed. Red{nbsp}Hat strongly recommends that etcd use a dedicated volume. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/scalability_and_performance/recommended-performance-and-scalability-practices-2#recommended-etcd-practices_recommended-etcd-practices[Recommended performance and scalability practices]. (link:https://issues.redhat.com/browse/OCPBUGS-73833[OCPBUGS-73833]) 
